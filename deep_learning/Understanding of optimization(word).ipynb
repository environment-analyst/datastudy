{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7670c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536ba83",
   "metadata": {},
   "source": [
    "# 최적화의 주요 용어 이해하기\n",
    "\n",
    "1. 최적화와 관련된 주요한 용어\n",
    "\n",
    "2. 다양한 Gradient Descent 기법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412a708",
   "metadata": {},
   "source": [
    "## 최적화\n",
    "\n",
    "1. Generalization\n",
    "    * 일반화 성능을 높이는 것이 중요함\n",
    "    * 일반화가 좋다 -> 학습데이터 에러와 테스트데이터에러의 차이가 적다\n",
    "\n",
    "\n",
    "2. Overfitting\n",
    "\n",
    "   * Underfitiing : 학습데이터 조차 잘 못 맞히는 것\n",
    "   \n",
    "   * Overfiting : 학습데이터에 대해서 성능(분류와 같은)이 좋지만 테스트데이터에 대해서 성능이 좋지 않은것\n",
    "    \n",
    "\n",
    "3. Cross-validation\n",
    "    * cross-validation으로 하이퍼파라미터를 찾음\n",
    "    * test data를 사용하는 것은 cheating\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af513f",
   "metadata": {},
   "source": [
    "4. Bias and Variance\n",
    "    * variance : 얼마나 일관되게 출력이 나오느냐, 간단한 모델들이 variance가 낮다, varince가 낮은 모델이 좋은 것, variance가 크다 -> 오버피팅될 가능성이 크다\n",
    "    * Bias : 편향 -> 내가 원하는 타겟과 얼마나 벗어나는가에 대한 내용\n",
    "\n",
    "\n",
    "5. Bias and Variance Tradeoff\n",
    "    * 내 학습데이터에 노이즈가 껴있다고 했을때, 이 값은 세 가지로 구성되어 있다,Bias, variance, noise 따라서 이 값 중 하나를 줄이려고 할 때 다른 값은 커질 수 밖에 없다고 함\n",
    "\n",
    "\n",
    "6. Bootstrapping\n",
    "    * 한개의 학습데이터 100가 있을 때, 80개 만을 선별하여 여러개의 모델을 만들어 데이터를 예측(또는 분류)하려고 하는것</br></br>\n",
    "\n",
    "7. Bagging vs. Boosting</br></br>\n",
    "    1. 배깅\n",
    "    * 배깅을 일반적으로 앙상블이라고 함\n",
    "    * 위에서 부트스트래핑으로 여러개의 모델을 만든 후 그 모델들로 나오는 아웃풋의 평균을 말하는 것</br>\n",
    "    </br>\n",
    "    2. 부스팅\n",
    "    * 100개의 학습데이터 중 80개만을 선별하여 모델을 만든 후 나머지 20개의 데이터를 돌려서 모델(모델이 잘 맞지 않는다면)을 만듦\n",
    "![baaging&boosting](baggig,boosting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2418643",
   "metadata": {},
   "source": [
    "# Graiden Descent\n",
    "\n",
    "1차 미분한 값만 사용하게 됨 극소값을 찾기 위해 \n",
    "</br>\n",
    "</br>\n",
    "\n",
    "## Practical Gradient Descent Methods\n",
    "\n",
    "1. Stochastic gradient descent\n",
    "* 하나의 샘플에 대해서마 그라디언트를 구해서 업데이트 하는 것\n",
    "\n",
    "2. Mini-batch gradient descent\n",
    "* 하나의 샘플에서 작은 크기의 데이터셋을 만들어 그라디언트를 구해 업데이트 하는 것\n",
    "\n",
    "3. Batch gradient \n",
    "* 하나의 샘플의 모든 데이터를 사용하여 그라디언트를 구해 업데이는 하는 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f185e",
   "metadata": {},
   "source": [
    "## Batch-size Matters\n",
    "\n",
    "* sharp minimizers \n",
    "\n",
    "\n",
    "* flat minimizers : 일반화가 높은 것\n",
    "\n",
    "batch-size가 적을 수록 일반화가 높아 성능이 더 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a164a2",
   "metadata": {},
   "source": [
    "## Gradient Descent Methods\n",
    "다 자동으로 그라디언트 디센트를 만들어줌\n",
    "\n",
    "1. SGD(Stochastic gradient descent) ?\n",
    "\n",
    "\n",
    "2. Momentum ?\n",
    "\n",
    "\n",
    "3. Nesterov accelerated gradient ?\n",
    "\n",
    "\n",
    "4. Adagrad ?\n",
    "\n",
    "\n",
    "5. Adadelta ?\n",
    "\n",
    "\n",
    "6. RMSprop ?\n",
    "\n",
    "\n",
    "7. Adam ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef658b8",
   "metadata": {},
   "source": [
    "\n",
    "Practical\n",
    "실용적인\n",
    "실지의\n",
    "generalization\n",
    "일반화\n",
    "\n",
    "\n",
    "estimator\n",
    "추정량\n",
    "\n",
    "\n",
    "automatic differentiation\n",
    "자동 미분\n",
    "\n",
    "\n",
    "momentum\n",
    "운동량\n",
    "\n",
    "\n",
    "window size\n",
    "한번에 받을 수 있는 데이터의 양\n",
    "\n",
    "\n",
    "exponential moving average\n",
    "지수이동평균\n",
    "\n",
    "\n",
    "Unbiased Estimator\n",
    "편의추정량은 통계학에서 기댓값이 모수와 다른 추정량이다.\n",
    "\n",
    "\n",
    "Unbiased Estimator 는 파라미터 추정 평균에 대해서 bias 값이 0인 경우를 말하고,\n",
    "Biased Estimator 는 파라미터 추정 평균의 bias값이 0이 아닌 경우를 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfa3b0",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "학습을 방행하는 것(제한하는 것) -> Overfiting을 방지하기 위해 사용\n",
    "\n",
    "1. Early stopping\n",
    "* 학습을 멈추는 것, 이때 사용하는 데이터는 validation data이다\n",
    "* 트페이닝 에러와 밸리데이션 에러 사이의 격차가 커지면 학습을 멈춘다\n",
    "\n",
    "\n",
    "2. Parameter Norm Penalty \n",
    "* 뉴럴 네트워크의 파라미터가 너무 커지지 않게 조정\n",
    "* 부드러운 함수 일수록\n",
    "\n",
    "\n",
    "3. Data Augmentation\n",
    "* 학습데이터가 많은 것은 항상 좋다\n",
    "* 하지만 트레이닝 데이터는 사전에 주어지기 때문에 이 경우 데이터 증량이 필요\n",
    "* 따라서 일부러 기울여서 새로운 데이터를 만드는 것임(단, 레이블이 변환되지 않는 한에서 변환)\n",
    "\n",
    "\n",
    "\n",
    "4. Noise Robustness \n",
    "* 일부러 인풋이나 웨이트에 노이즈를 넣음\n",
    "\n",
    "\n",
    "5. Lebel Smoothing \n",
    "* Mix-up: 이미지를 다른 이미지 라벨과 섞는것(블렌딩? 오버랩 하듯이 섞음)\n",
    "\n",
    "* Cutout: 이미지를 잘라내어 손실을 만들어 내는 것\n",
    "\n",
    "* CutMix: 이미지를 블렌딩이 아니라 꼴라쥬처럼 섞는것\n",
    "\n",
    "\n",
    "\n",
    "6. Dropout\n",
    "?\n",
    "\n",
    "\n",
    "7. Batch Normalization \n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9353c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
